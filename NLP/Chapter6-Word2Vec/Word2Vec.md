## word2vec:CBOW(连续词袋)  
由两边的n个词，推断中间的一个词（去掉了神经网络的hidden layers）  
* 无hidden layers
* 使用双向上下文窗口
* 上下文词序无关（Bow）
* 输入层直接使用低维稠密表示
* 投影层简化为求和（平均）  
为了解决softmax分类函数计算量大的问题（负样本太大），提出了2个方案：  
#### 1.CBOW：层次softmax  
使用huffman tree来编码输出层的词典：softmax变成多个sigmoid  
#### 2.CBOW:负采样  
一个正样本，V-1个负样本，对负样本进行采样  
如何采样？  
保证采样样本的数据和正文本有相似度  
比如采集999个负样本，则softmax只需要分类1000种，大大降低softmax的分类个数  
词典中每一个词对应一条线段，所有词划分[0,1]个等分
