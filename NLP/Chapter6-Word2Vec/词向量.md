## NLP常见任务  
* 自动摘要
* 指代消解
* 机器翻译
* 词性标注
* 分词
* 主题识别
* 文本分类

## 词编码  
**词编码需要保证词的相关性**  

简单词的向量空间分布有相似性
## 词的离散表示
#### Bag of Word  
文档的向量表示可以直接将各词的词向量表示加和（出现频率的加和）  
#### 词权重（**词在文档中的顺序没有被考虑**）
* TF-IDF  
词t的IDF Weight： log（1+N/nt） N:文档总数   nt：含有词t的文档数
* Binary Weighting  
短文本相似性：Berinoulli Naive Bayes  
#### Bi-gram or N-gram  
优点：考虑了词的顺序  
缺点：词表膨胀，计算量增大  
语言模型：  
一句话出现的概率  

![PicName](https://github.com/jiaruncao/jiaruncao.github.io/blob/master/NLP/Chapter6-Word2Vec/formula/21.png)  
以bi-gram为例：  

![PicName](https://github.com/jiaruncao/jiaruncao.github.io/blob/master/NLP/Chapter6-Word2Vec/formula/22.png)  

### 离散表示的问题
* 无法衡量词向量之间的关系（太稀疏，很难捕捉文本的关系）  
* 词表维度随着语料库增长膨胀 
* n-gram词序列随语料库膨胀更快
* 数据稀疏的问题
## 词的分布式表示  
“用一个词附近的其他词来表示该词”  

![PicName](https://github.com/jiaruncao/jiaruncao.github.io/blob/master/NLP/Chapter6-Word2Vec/formula/23.png)  

### 共现矩阵
Word-Document的共现矩阵主要是用于发现主题（LSA）  
局域窗中的word-word共现矩阵可以挖掘语法和语义信息  
**window length 设1（一般为5-10）**  
使用对称的窗函数（左右的window length都为1）  

![PicName](https://github.com/jiaruncao/jiaruncao.github.io/blob/master/NLP/Chapter6-Word2Vec/formula/24.png)  

#### 存在的问题：  
* 向量维数随着向量大小线性增长
* 存储整个词典的空间消耗非常大
* 一些模型如文本分类模型会面临稀疏的问题
* 模型可能欠稳定  
因此想到用SVD降维  
## NNLM(Neural Network Language Model)  
直接从语言模型出发，将模型最优化过程转换为求词向量表示的过程  
目标函数：  

* 使用了非对称的前向窗函数，窗长度为n-1  
* 滑动窗口遍历整个语料库求和，计算量正比于语料库大小  
* 概率P满足归一化条件，这样不同位置t处的概率才能相加

