# 朴素贝叶斯
很多高级自然语言处理模型从它演化而来。学习贝叶斯方法，是研究自然语言处理问题的一个非常好的切入口
## 1. 贝叶斯公式  
![PicName](https://github.com/jiaruncao/jiaruncao.github.io/blob/master/NLP/Chapter2-language%20model%20and%20Naive%20Bayes/formula/1.png)  
  
从贝叶斯公式的发现历史来看，其就是为了处理所谓“逆概”问题而诞生的。比如 P(Y|X)  不能通过直接观测来得到结果，而 P(X|Y)  却容易通过直接观测得到结果，就可以通过贝叶斯公式从间接地观测对象去推断不可直接观测的对象的情况。  
  
![PicName](https://github.com/jiaruncao/jiaruncao.github.io/blob/master/NLP/Chapter2-language%20model%20and%20Naive%20Bayes/formula/3.png)  

其中  ∝  表示“正比于”。而 P(X|Yi)  则有个特别高逼格的名字叫做“似然函数”。我们上大学的时候也被这个名字搞得晕晕乎乎的，其实它也是个概率，直接理解成“ P(Yi|X)  的逆反条件概率” 就方便了。
  
#### 独立假设
我们把 X 理解成“**具有某特征**”，将 Y 理解成“**属于某类**”的标签，则：  

![PicName](https://github.com/jiaruncao/jiaruncao.github.io/blob/master/NLP/Chapter2-language%20model%20and%20Naive%20Bayes/formula/4.png)  
  
  
#### 贝叶斯分类器  
回到机器学习，其实我们就是对每个类别计算一个概率 p(ci) ，然后再计算所有特征的条件概率 p(fj|ci) ，那么分类的时候我们就是依据贝叶斯找一个最可能的类别：  
  
  
![PicName](https://github.com/jiaruncao/jiaruncao.github.io/blob/master/NLP/Chapter2-language%20model%20and%20Naive%20Bayes/formula/5.png)  

## 2. 词袋模型（BOW）
但由于乘法交换律，词语顺序不同时，朴素贝叶斯方法中算出来二者的条件概率完全一样。比如，在条件独立假设的情况下，“武松打死了老虎”与“老虎打死了武松”被它认作一个意思。因此说，朴素贝叶斯失去了词语之间的顺序信息。这就相当于把所有的词汇扔进到一个袋子里随便搅和，贝叶斯都认为它们一样。因此这种情况也称作**词袋模型(bag of words)**。  
  
  
**[tricks]**：联合概率有很多乘法运算，计算的时间开销比较大。一般只求概率的log值，这样可以将连乘变为连加  

## 3. 处理重复词的3种方法
#### 多项式模型
考虑重复词语的情况，重复的词语我们视为其出现多次，直接按条件独立假设的方式推导，将多次出现的词语相乘即可
#### 伯努利模型
将重复的词语都视为其只出现1次，只计算一次条件概率
#### 混合模型
在计算句子概率时，不考虑重复词语出现的次数，但是在统计计算词语的概率P(“词语”|S）时，却考虑重复词语的出现次数，这样的模型可以叫作混合模型。

## 4. 去除停用词与选择关键词
像“我”、“的”之类词非常中性，无法帮助判断的有用信息。这些无助于我们分类的词语叫作“停用词”（Stop Words）。去掉可以减少我们训练模型、判断分类的时间。  
  
  
以人类的经验，针对某一话题很有代表性的词语就是关键词。例如垃圾邮件中，“正规发票”、“发票”这类的词如果出现的话，邮件作为垃圾邮件的概率非常大，可以作为我们区分垃圾邮件的“关键词”。  
    
**“停用词”和“关键词”一般都可以提前靠人工经验指定**。不同的“停用词”和“关键词”训练出来的分类器的效果也会有些差异。

## 5. Laplace平滑
如果某一个条件概率为0，可能会导致联合概率相乘时整体为0，为了解决这种问题，提出了平滑技术。 
对于伯努利模型，P(“正规发票”|S）的一种平滑算法是：  

![PicName](https://github.com/jiaruncao/jiaruncao.github.io/blob/master/NLP/Chapter2-language%20model%20and%20Naive%20Bayes/formula/6.png)  

对于多项式模型，P(“正规发票”| S）的一种平滑算法是：  

![PicName](https://github.com/jiaruncao/jiaruncao.github.io/blob/master/NLP/Chapter2-language%20model%20and%20Naive%20Bayes/formula/7.png)  

平滑技术的种类其实非常多，就是所有的平滑技术都是**给未出现在训练集中的词语一个估计的概率，而相应地调低其他已经出现的词语的概率**。  
  
  
平滑技术是因为数据集太小而产生的现实需求。如果数据集足够大，平滑技术对结果的影响将会变小。

## 6. 小结
贝叶斯分类器的基本思路是先区分好训练集与测试集，对文本集合进行分词、去除标点符号等特征预处理的操作，然后使用条件独立假设，将原概率转换成词概率乘积，再进行后续的处理。  
  
  

贝叶斯公式 + 条件独立假设 = 朴素贝叶斯方法  
  
  
基于对重复词语在训练阶段与判断（测试）阶段的三种不同处理方式，我们相应的有伯努利模型、多项式模型和混合模型。在训练阶段，如果样本集合太小导致某些词语并未出现，我们可以采用平滑技术对其概率给一个估计值。而且并不是所有的词语都需要统计，我们可以按相应的“停用词”和“关键词”对模型进行进一步简化，提高训练和判断速度。  






