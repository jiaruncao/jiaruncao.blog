## 1. 引言：朴素贝叶斯的局限性
我们知道朴素贝叶斯的局限性来源于其条件独立假设，它将文本看成是词袋子模型，不考虑词语之间的顺序信息，就会把“武松打死了老虎”与“老虎打死了武松”认作是一个意思。那么有没有一种方法提高其对词语顺序的识别能力呢？有，就是这里要提到的N-gram语言模型。  

## 2. N-gram语言模型  
为了简化起见，我们以字母 xi 表示每一个词语，并且先不考虑条件“S”，则独立假设公式：  

![PicName](https://github.com/jiaruncao/jiaruncao.github.io/blob/master/NLP/Chapter2-language%20model%20and%20Naive%20Bayes/formula/8.png)  
### 独立假设与联合概率  

由此可以看出，采用独立假设公式，由于乘法的交换律，词和词之间没有依赖性  

上面的公式要求满足独立性假设，为了满足词和词之间的依赖关系，我们应该有下面这个恒等式，即**联合概率链规则(chain rule)** ：  
  
  
![PicName](https://github.com/jiaruncao/jiaruncao.github.io/blob/master/NLP/Chapter2-language%20model%20and%20Naive%20Bayes/formula/9.png) 


### N-gram语言模型  
上面的联合概率链规则公式考虑到词和词之间的依赖关系，但是比较复杂，在实际生活中几乎没办法使用，于是我们就想了很多办法去近似这个公式，比如我们语言模型n-gram就是它的一个简化。  

如果我们只考虑一个词语对上一个词语的依赖关系，公式就简化了如下形式，我们把它叫做二元语法（bigram，2-gram）:  
  
  
![PicName](https://github.com/jiaruncao/jiaruncao.github.io/blob/master/NLP/Chapter2-language%20model%20and%20Naive%20Bayes/formula/10.png)  

  
如果我们再考虑长一点，考虑n个词语之间的关系,这就是n-gram的由来。 

其实以上几个简化后的公式，就是著名的**马尔科夫假设（Markov Assumption）：下一个词的出现仅依赖于它前面的一个或几个词**。这相对于联合概率链规则，其实是一个有点粗糙的简化，不过很好地体现了就近思路，离得较远和关系比较弱的词语就被简化和省略了。实际应用中，这些简化后的n-gram语法比独立性假设还是强很多的。  
  
### N-gram语言模型中，N如何选择？

理论上，只要有足够大的语料，n越大越好，这样考虑的信息更多。条件概率很好算，统计一下各个元组出现的次数就可以，比如:  

![PicName](https://github.com/jiaruncao/jiaruncao.github.io/blob/master/NLP/Chapter2-language%20model%20and%20Naive%20Bayes/formula/11.png)  
  
  
但我们实际情况往往是**训练语料很有限，很容易产生数据稀疏**，不满足大数定律，算出来的概率失真。  
  
那么，如何选择依赖词的个数n呢？从前人的经验来看：  

* 经验上，trigram用的最多。尽管如此，原则上，能用bigram解决，绝不使用trigram。n取≥4的情况较少。
* 更大的n：对下一个词出现的约束信息更多，具有更大的辨别力；
* 更小的n：在训练语料库中出现的次数更多，具有更可靠的统计信息，具有更高的可靠性、实用性。  

## 3. N-gram语言模型应用  

#### * 词性标注  

词性标注是一个典型的多分类问题，而一个词可能属于多种词性。如“爱”，可能是动词，可能是形容词，也可能是名词。考虑到词性会受前面一两个词的词性的影响，可以引入2-gram模型提升匹配的精确度。 我们匹配以下这句话（已被空格分好词）中“爱”的词性：  
  
>  "闷骚的 李雷 很 爱 韩梅梅"  
  
将公式进行以下改造，比较各概率的大小，选择概率最大的词性：  


![PicName](https://github.com/jiaruncao/jiaruncao.github.io/blob/master/NLP/Chapter2-language%20model%20and%20Naive%20Bayes/formula/12.png)   

  
PS:词性标注是自然语言处理中的一项基础性工作，感兴趣可以看看这篇文章[《NLTK读书笔记 — 分类与标注》](https://superangevil.wordpress.com/2009/10/20/nltk5/)  
  
#### * 垃圾邮件识别
#### * 中文分词
#### * 机器翻译
#### * 语音识别  

## 4. 平滑技术
现在我们比较专门探讨平滑技术。平滑技术只要是为了解决零概率问题，我们需要给 “未出现的n-gram条件概率分布一个非零估计值，相应得需要降低已出现n-gram条件概率分布，且经数据平滑后一定保证概率和为1”。这就是平滑技术的基本思想  
#### 4.1 拉普拉斯平滑
这是最古老的一种平滑方法，又称加一平滑法，其**保证每个n-gram在训练语料中至少出现1次**。以计算概率 P(“优惠”|“发票”,“点数”)为例，公式如下：  
![PicName](https://github.com/jiaruncao/jiaruncao.github.io/blob/master/NLP/Chapter2-language%20model%20and%20Naive%20Bayes/formula/13.png)     
在所有不重复的三元组的个数远大于(“发票”,“点数”)出现的次数时，即训练语料库中绝大部分n-gram都是未出现的情况（一般都是如此），拉普拉斯平滑有“喧宾夺主”的现象，效果不佳  
#### 4.2 古德图灵(Good Turing)平滑  
通过对语料库的统计，我们能够知道出现r次（r>0的n元组的个数为Nr，可以令从未出现的n元组的个数为N0，古德图灵平滑的思想是:
* 出现0次的n元组也不能认为其是0次，应该给它一个比较小的估计值，比如为 d0 次。
* 为了保证总共的（出现和未出现的）n元组的次数不变，其他所有已出现的n元组的次数r应该打一个折扣，比如为 dr 次。
* 然后再用新的 dr 去计算各个条件概率。  
所以问题的关键是计算 dr 。为了保证平滑前后n元组的总共出现次数不变，有：  
![PicName](https://github.com/jiaruncao/jiaruncao.github.io/blob/master/NLP/Chapter2-language%20model%20and%20Naive%20Bayes/formula/14.png) 
